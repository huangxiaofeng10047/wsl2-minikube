
==> Audit <==
|---------|--------------------------------|----------|---------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |  User   | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|---------|---------|---------------------|---------------------|
| start   | --memory=8096 --cpus=4         | minikube | xfhuang | v1.28.0 | 11 Jan 23 14:01 CST |                     |
|         | --insecure-registry            |          |         |         |                     |                     |
|         | 10.7.20.12:5000,10.7.20.51     |          |         |         |                     |                     |
|         | --image-mirror-country=cn      |          |         |         |                     |                     |
| addons  | enable dashboard               | minikube | xfhuang | v1.28.0 | 11 Jan 23 14:10 CST |                     |
| ssh     |                                | minikube | xfhuang | v1.28.0 | 11 Jan 23 14:11 CST | 11 Jan 23 14:12 CST |
| addons  | enable dashboard               | minikube | xfhuang | v1.28.0 | 11 Jan 23 14:12 CST |                     |
|---------|--------------------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2023/01/11 14:01:17
Running on machine: coderhuang
Binary: Built with gc go1.19.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0111 14:01:17.576819 3502553 out.go:296] Setting OutFile to fd 1 ...
I0111 14:01:17.576942 3502553 out.go:309] Setting ErrFile to fd 2...
I0111 14:01:17.577011 3502553 root.go:334] Updating PATH: /home/xfhuang/.minikube/bin
W0111 14:01:17.577092 3502553 root.go:311] Error reading config file at /home/xfhuang/.minikube/config/config.json: open /home/xfhuang/.minikube/config/config.json: no such file or directory
I0111 14:01:17.577325 3502553 out.go:303] Setting JSON to false
I0111 14:01:17.577798 3502553 start.go:116] hostinfo: {"hostname":"coderhuang","uptime":75351,"bootTime":1673341527,"procs":45,"os":"linux","platform":"arch","platformFamily":"arch","platformVersion":"\"rolling\"","kernelVersion":"5.10.102.1-microsoft-hxf-standard-WSL2+","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"11e47d8c-8b24-6615-0a15-119a62b4385a"}
I0111 14:01:17.577838 3502553 start.go:126] virtualization:  guest
I0111 14:01:17.665008 3502553 out.go:177] üòÑ  minikube v1.28.0 on Arch "rolling" (amd64)
W0111 14:01:17.731928 3502553 preload.go:295] Failed to list preload files: open /home/xfhuang/.minikube/cache/preloaded-tarball: no such file or directory
I0111 14:01:17.731991 3502553 notify.go:220] Checking for updates...
I0111 14:01:17.732106 3502553 driver.go:365] Setting default libvirt URI to qemu:///system
I0111 14:01:17.732124 3502553 global.go:111] Querying for installed drivers using PATH=/home/xfhuang/.minikube/bin:/home/xfhuang/.gvm/pkgsets/go1.19.3/global/bin:/home/xfhuang/.gvm/gos/go1.19.3/bin:/home/xfhuang/.gvm/pkgsets/go1.19.3/global/overlay/bin:/home/xfhuang/.gvm/bin:/home/xfhuang/.gvm/bin:/home/xfhuang/.antigen/bundles/paulirish/git-open:/home/xfhuang/.gvm/pkgsets/go1.19.3/global/bin:/home/xfhuang/.gvm/gos/go1.19.3/bin:/home/xfhuang/.gvm/pkgsets/go1.19.3/global/overlay/bin:/home/xfhuang/.gvm/bin:/home/xfhuang/.nvm/versions/node/v16.15.0/bin:/home/xfhuang/.antigen/bundles/paulirish/git-open:/opt/distrod/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/WindowsApps/Microsoft.WindowsTerminal_1.15.3466.0_x64__8wekyb3d8bbwe:/mnt/d/Users/Administrator/App/Microsoft/jdk-17.0.4.8-hotspot/bin:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/dotnet/:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/nodejs/:/mnt/c/Program Files/VSCodium/bin:/mnt/c/Users/Administrator/scoop/apps/vscodium/current/bin:/mnt/c/Users/Administrator/scoop/shims:/mnt/c/Users/Administrator/AppData/Local/Microsoft/WindowsApps:/mnt/c/ProgramFiles/JetBrains/IntelliJ IDEA 2021.2.3/bin:/mnt/c/Program Files/JetBrains/GoLand 2021.2.4/bin:/mnt/d/software/Sublime Text:/mnt/c/Program Files/JetBrains/IntelliJ IDEA 2021.3.2/bin:/mnt/d/software/apache-maven-3.8.5/bin:/mnt/c/Java/apache-maven-3.2.2/bin:/mnt/c/Users/Administrator/AppData/Roaming/npm:/mnt/c/Users/Administrator/AppData/Local/JetBrains/Toolbox/scripts:/mnt/c/Java/jbr_jcef-17.0.3-x64-b469/bin:/mnt/c/Users/Administrator/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/d/Users/Administrator/AppData/Local/Programs/Hyper/resources/bin:/mnt/c/Users/Administrator/scoop/apps/hyper/3.2.3/resources/bin:/home/xfhuang/.dotnet/tools:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/lib:/home/xfhuang/.antigen/bundles/zsh-users/zsh-syntax-highlighting:/home/xfhuang/.antigen/bundles/zsh-users/zsh-autosuggestions:/home/xfhuang/.antigen/bundles/zsh-users/zsh-completions:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/git:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/heroku:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/pip:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/lein:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/command-not-found:/home/xfhuang/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/z:/home/xfhuang/workspace/go/bin:/home/xfhuang/.local/share/JetBrains/Toolbox/scripts:/home/xfhuang/.local/share/gem/ruby/3.0.0/bin:/home/xfhuang/.local/bin:/home/xfhuang/workspace/go/bin:/home/xfhuang/.local/share/JetBrains/Toolbox/scripts:/home/xfhuang/.local/share/gem/ruby/3.0.0/bin:/home/xfhuang/.local/bin
I0111 14:01:17.747198 3502553 global.go:119] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0111 14:01:18.052164 3502553 podman.go:123] podman version: 4.3.1
I0111 14:01:18.052184 3502553 global.go:119] podman default: true priority: 7, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0111 14:01:18.117037 3502553 global.go:119] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0111 14:01:18.117080 3502553 global.go:119] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0111 14:01:18.255114 3502553 global.go:119] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0111 14:01:18.312789 3502553 global.go:119] vmware default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker-machine-driver-vmware": executable file not found in $PATH Reason: Fix:Install docker-machine-driver-vmware Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0111 14:01:18.332673 3502553 docker.go:137] docker version: linux-20.10.22
I0111 14:01:18.332748 3502553 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0111 14:01:18.388209 3502553 info.go:266] docker info: {ID:6H65:4PEL:RWEQ:5QQN:CKUF:VCVD:3OWM:KJNP:V5UU:5LA3:VXP5:D6KJ Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:152 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:40 SystemTime:2023-01-11 14:01:18.3483873 +0800 CST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.10.102.1-microsoft-hxf-standard-WSL2+ OperatingSystem:Arch Linux OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[0.0.0.0/0 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:16778825728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:coderhuang Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323.m Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323.m} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:2.14.2]] Warnings:<nil>}}
I0111 14:01:18.388264 3502553 docker.go:254] overlay module found
I0111 14:01:18.388272 3502553 global.go:119] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0111 14:01:18.453747 3502553 global.go:119] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I0111 14:01:18.453773 3502553 driver.go:300] not recommending "none" due to default: false
I0111 14:01:18.453777 3502553 driver.go:300] not recommending "ssh" due to default: false
I0111 14:01:18.453790 3502553 driver.go:335] Picked: docker
I0111 14:01:18.453794 3502553 driver.go:336] Alternatives: [podman none ssh]
I0111 14:01:18.453798 3502553 driver.go:337] Rejects: [qemu2 virtualbox vmware kvm2]
I0111 14:01:18.548421 3502553 out.go:177] ‚ú®  Automatically selected the docker driver. Other choices: podman, none, ssh
I0111 14:01:18.631668 3502553 start.go:282] selected driver: docker
I0111 14:01:18.631685 3502553 start.go:808] validating driver "docker" against <nil>
I0111 14:01:18.631711 3502553 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0111 14:01:18.632244 3502553 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0111 14:01:18.673770 3502553 info.go:266] docker info: {ID:6H65:4PEL:RWEQ:5QQN:CKUF:VCVD:3OWM:KJNP:V5UU:5LA3:VXP5:D6KJ Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:152 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:40 SystemTime:2023-01-11 14:01:18.6475218 +0800 CST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.10.102.1-microsoft-hxf-standard-WSL2+ OperatingSystem:Arch Linux OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[0.0.0.0/0 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:16778825728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:coderhuang Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323.m Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323.m} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:2.14.2]] Warnings:<nil>}}
I0111 14:01:18.673830 3502553 start_flags.go:303] no existing cluster config was found, will generate one from the flags 
I0111 14:01:18.673926 3502553 start.go:878] selecting image repository for country cn ...
I0111 14:01:18.981603 3502553 out.go:177] ‚úÖ  Using image repository registry.cn-hangzhou.aliyuncs.com/google_containers
I0111 14:01:19.048226 3502553 start_flags.go:883] Wait components to verify : map[apiserver:true system_pods:true]
I0111 14:01:19.115307 3502553 out.go:177] üìå  Using Docker driver with root privileges
I0111 14:01:19.181431 3502553 cni.go:95] Creating CNI manager for ""
I0111 14:01:19.181446 3502553 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0111 14:01:19.181454 3502553 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:8096 CPUs:4 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.7.20.12:5000 10.7.20.51] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/xfhuang:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0111 14:01:19.284199 3502553 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0111 14:01:19.348272 3502553 cache.go:120] Beginning downloading kic base image for docker with docker
I0111 14:01:19.423295 3502553 out.go:177] üöú  Pulling base image ...
I0111 14:01:19.490062 3502553 image.go:76] Checking for registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon
I0111 14:01:19.490092 3502553 cache.go:107] acquiring lock: {Name:mk00de3921f3a767ca73d420a2fd7e8e5b21d93e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490109 3502553 cache.go:107] acquiring lock: {Name:mk9368b2f1d151226a43ab3d336f850139f352a6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490159 3502553 cache.go:107] acquiring lock: {Name:mk716f6b4e66211e094683e5d47a9c63c0221b1b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490162 3502553 cache.go:107] acquiring lock: {Name:mkc35d0312146bb9385a79c85bb44842f9dc84bd Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490165 3502553 cache.go:107] acquiring lock: {Name:mk8109f5afa36ae36bd78d13dc064019450f7193 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490191 3502553 profile.go:148] Saving config to /home/xfhuang/.minikube/profiles/minikube/config.json ...
I0111 14:01:19.490190 3502553 cache.go:107] acquiring lock: {Name:mk29dc876239056c77563847700483cf4147e011 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490193 3502553 cache.go:107] acquiring lock: {Name:mk9b24ecc170c70a8f8c057b250eb9256c7d1ccc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490206 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/config.json: {Name:mkdc24424a650e96125dec44aaf9ead754d1d966 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:01:19.490218 3502553 cache.go:107] acquiring lock: {Name:mk29dd065d34cf1283567059e8a68f8ae43acca4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.490534 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:19.490534 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:19.490535 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:19.490534 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:19.490549 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:19.490626 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:19.490706 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:19.490777 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:19.491143 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:19.491156 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:19.491174 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:19.491190 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:19.491208 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:19.491212 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:19.491218 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:19.491717 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:19.540580 3502553 image.go:80] Found registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon, skipping pull
I0111 14:01:19.540607 3502553 cache.go:142] registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 exists in daemon, skipping load
I0111 14:01:19.540634 3502553 cache.go:208] Successfully downloaded all kic artifacts
I0111 14:01:19.540689 3502553 start.go:364] acquiring machines lock for minikube: {Name:mk8140d8819b4b62a3a6e8656e1e29611e6220ea Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0111 14:01:19.540752 3502553 start.go:368] acquired machines lock for "minikube" in 50.9¬µs
I0111 14:01:19.540774 3502553 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:8096 CPUs:4 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.7.20.12:5000 10.7.20.51] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/xfhuang:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet} &{Name: IP: Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0111 14:01:19.541237 3502553 start.go:125] createHost starting for "" (driver="docker")
I0111 14:01:19.698487 3502553 out.go:204] üî•  Creating docker container (CPUs=4, Memory=8096MB) ...
I0111 14:01:19.699171 3502553 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0111 14:01:19.699291 3502553 client.go:168] LocalClient.Create starting
I0111 14:01:19.699483 3502553 main.go:134] libmachine: Creating CA: /home/xfhuang/.minikube/certs/ca.pem
I0111 14:01:19.741024 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8
I0111 14:01:19.759837 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3
I0111 14:01:19.786476 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3
I0111 14:01:19.798938 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5
I0111 14:01:19.804759 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3
I0111 14:01:19.808515 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3
I0111 14:01:19.816987 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0
I0111 14:01:19.872805 3502553 cache.go:161] opening:  /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3
I0111 14:01:19.945411 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8 exists
I0111 14:01:19.945468 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8" took 455.2761ms
I0111 14:01:19.945479 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8 succeeded
I0111 14:01:20.081439 3502553 main.go:134] libmachine: Creating client certificate: /home/xfhuang/.minikube/certs/cert.pem
I0111 14:01:20.328980 3502553 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0111 14:01:20.351052 3502553 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0111 14:01:20.351160 3502553 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I0111 14:01:20.351173 3502553 cli_runner.go:164] Run: docker network inspect minikube
W0111 14:01:20.372210 3502553 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0111 14:01:20.372223 3502553 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I0111 14:01:20.372231 3502553 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I0111 14:01:20.372287 3502553 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0111 14:01:20.394881 3502553 network.go:295] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc00095e008] misses:0}
I0111 14:01:20.394983 3502553 network.go:241] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I0111 14:01:20.394998 3502553 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0111 14:01:20.395137 3502553 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0111 14:01:20.737997 3502553 network_create.go:99] docker network minikube 192.168.49.0/24 created
I0111 14:01:20.738012 3502553 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I0111 14:01:20.738196 3502553 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0111 14:01:20.756573 3502553 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0111 14:01:20.807718 3502553 oci.go:103] Successfully created a docker volume minikube
I0111 14:01:20.807854 3502553 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 -d /var/lib
I0111 14:01:27.194857 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 exists
I0111 14:01:27.194879 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5" took 7.7047924s
I0111 14:01:27.194890 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 succeeded
I0111 14:01:27.798012 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3 exists
I0111 14:01:27.798040 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3" took 8.3078935s
I0111 14:01:27.798052 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3 succeeded
I0111 14:01:28.356279 3502553 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 -d /var/lib: (7.5483983s)
I0111 14:01:28.356292 3502553 oci.go:107] Successfully prepared a docker volume minikube
I0111 14:01:28.356320 3502553 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
W0111 14:01:28.438387 3502553 preload.go:115] https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube-preloaded-volume-tarballs/v18/v1.25.3/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 status code: 404
W0111 14:01:28.438478 3502553 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0111 14:01:28.438595 3502553 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0111 14:01:28.535723 3502553 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8096mb --cpus=4 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456
I0111 14:01:28.785980 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3 exists
I0111 14:01:28.786098 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3" took 9.295879s
I0111 14:01:28.786266 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3 succeeded
I0111 14:01:32.185817 3502553 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8096mb --cpus=4 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456: (3.6500375s)
I0111 14:01:32.185882 3502553 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0111 14:01:32.220368 3502553 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0111 14:01:32.246337 3502553 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0111 14:01:32.283401 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3 exists
I0111 14:01:32.283425 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3" took 12.7933212s
I0111 14:01:32.283435 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3 succeeded
I0111 14:01:32.344188 3502553 oci.go:144] the created container "minikube" has a running status.
I0111 14:01:32.344203 3502553 kic.go:210] Creating ssh key for kic: /home/xfhuang/.minikube/machines/minikube/id_rsa...
I0111 14:01:32.393234 3502553 kic_runner.go:191] docker (temp): /home/xfhuang/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0111 14:01:32.473726 3502553 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0111 14:01:32.497807 3502553 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0111 14:01:32.497817 3502553 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0111 14:01:32.584578 3502553 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0111 14:01:32.713303 3502553 machine.go:88] provisioning docker machine ...
I0111 14:01:32.713323 3502553 ubuntu.go:169] provisioning hostname "minikube"
I0111 14:01:32.713404 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:32.740367 3502553 main.go:134] libmachine: Using SSH client type: native
I0111 14:01:32.740494 3502553 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0111 14:01:32.740502 3502553 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0111 14:01:32.740918 3502553 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59190->127.0.0.1:32787: read: connection reset by peer
I0111 14:01:34.568256 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3 exists
I0111 14:01:34.568279 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3" took 15.078117s
I0111 14:01:34.568309 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3 succeeded
I0111 14:01:35.069521 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3 exists
I0111 14:01:35.069561 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3" took 15.5793715s
I0111 14:01:35.069571 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3 succeeded
I0111 14:01:35.883289 3502553 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0111 14:01:35.883391 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:35.915723 3502553 main.go:134] libmachine: Using SSH client type: native
I0111 14:01:35.915852 3502553 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0111 14:01:35.915864 3502553 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0111 14:01:35.985143 3502553 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0111 14:01:35.985158 3502553 ubuntu.go:175] set auth options {CertDir:/home/xfhuang/.minikube CaCertPath:/home/xfhuang/.minikube/certs/ca.pem CaPrivateKeyPath:/home/xfhuang/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/xfhuang/.minikube/machines/server.pem ServerKeyPath:/home/xfhuang/.minikube/machines/server-key.pem ClientKeyPath:/home/xfhuang/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/xfhuang/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/xfhuang/.minikube}
I0111 14:01:35.985169 3502553 ubuntu.go:177] setting up certificates
I0111 14:01:35.985197 3502553 provision.go:83] configureAuth start
I0111 14:01:35.985538 3502553 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0111 14:01:36.012635 3502553 provision.go:138] copyHostCerts
I0111 14:01:36.012679 3502553 exec_runner.go:151] cp: /home/xfhuang/.minikube/certs/ca.pem --> /home/xfhuang/.minikube/ca.pem (1078 bytes)
I0111 14:01:36.012778 3502553 exec_runner.go:151] cp: /home/xfhuang/.minikube/certs/cert.pem --> /home/xfhuang/.minikube/cert.pem (1123 bytes)
I0111 14:01:36.012819 3502553 exec_runner.go:151] cp: /home/xfhuang/.minikube/certs/key.pem --> /home/xfhuang/.minikube/key.pem (1679 bytes)
I0111 14:01:36.012874 3502553 provision.go:112] generating server cert: /home/xfhuang/.minikube/machines/server.pem ca-key=/home/xfhuang/.minikube/certs/ca.pem private-key=/home/xfhuang/.minikube/certs/ca-key.pem org=xfhuang.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0111 14:01:36.099179 3502553 provision.go:172] copyRemoteCerts
I0111 14:01:36.099231 3502553 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0111 14:01:36.099271 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:36.120218 3502553 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/xfhuang/.minikube/machines/minikube/id_rsa Username:docker}
I0111 14:01:36.209646 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0111 14:01:36.243682 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0111 14:01:36.277622 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0111 14:01:36.312597 3502553 provision.go:86] duration metric: configureAuth took 327.3559ms
I0111 14:01:36.312610 3502553 ubuntu.go:193] setting minikube options for container-runtime
I0111 14:01:36.312795 3502553 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I0111 14:01:36.312873 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:36.345310 3502553 main.go:134] libmachine: Using SSH client type: native
I0111 14:01:36.345419 3502553 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0111 14:01:36.345426 3502553 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0111 14:01:36.460130 3502553 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0111 14:01:36.460140 3502553 ubuntu.go:71] root file system type: overlay
I0111 14:01:36.460378 3502553 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0111 14:01:36.460436 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:36.489070 3502553 main.go:134] libmachine: Using SSH client type: native
I0111 14:01:36.489211 3502553 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0111 14:01:36.489261 3502553 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=localhost,127.0.0.1,10.0.37.153,qua.io,10.7.116.12"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --insecure-registry 10.7.20.12:5000 --insecure-registry 10.7.20.51 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0111 14:01:36.585063 3502553 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=localhost,127.0.0.1,10.0.37.153,qua.io,10.7.116.12


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --insecure-registry 10.7.20.12:5000 --insecure-registry 10.7.20.51 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0111 14:01:36.585171 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:36.610530 3502553 main.go:134] libmachine: Using SSH client type: native
I0111 14:01:36.610687 3502553 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0111 14:01:36.610716 3502553 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0111 14:01:40.861045 3502553 cache.go:156] /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0 exists
I0111 14:01:40.861067 3502553 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0" -> "/home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0" took 21.3709304s
I0111 14:01:40.861079 3502553 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0 -> /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0 succeeded
I0111 14:01:40.861111 3502553 cache.go:87] Successfully saved all images to host disk.
I0111 14:01:42.407906 3502553 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-10-18 18:18:12.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-01-11 06:01:36.571240600 +0000
@@ -1,30 +1,33 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=NO_PROXY=localhost,127.0.0.1,10.0.37.153,qua.io,10.7.116.12
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --insecure-registry 10.7.20.12:5000 --insecure-registry 10.7.20.51 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +35,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0111 14:01:42.407922 3502553 machine.go:91] provisioned docker machine in 9.6946082s
I0111 14:01:42.407929 3502553 client.go:171] LocalClient.Create took 22.7086266s
I0111 14:01:42.407937 3502553 start.go:167] duration metric: libmachine.API.Create for "minikube" took 22.7087773s
I0111 14:01:42.407943 3502553 start.go:300] post-start starting for "minikube" (driver="docker")
I0111 14:01:42.407949 3502553 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0111 14:01:42.408011 3502553 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0111 14:01:42.408063 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:42.431271 3502553 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/xfhuang/.minikube/machines/minikube/id_rsa Username:docker}
I0111 14:01:42.518289 3502553 ssh_runner.go:195] Run: cat /etc/os-release
I0111 14:01:42.520694 3502553 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0111 14:01:42.520704 3502553 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0111 14:01:42.520710 3502553 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0111 14:01:42.520714 3502553 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0111 14:01:42.520722 3502553 filesync.go:126] Scanning /home/xfhuang/.minikube/addons for local assets ...
I0111 14:01:42.520754 3502553 filesync.go:126] Scanning /home/xfhuang/.minikube/files for local assets ...
I0111 14:01:42.520766 3502553 start.go:303] post-start completed in 112.8186ms
I0111 14:01:42.520964 3502553 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0111 14:01:42.540884 3502553 profile.go:148] Saving config to /home/xfhuang/.minikube/profiles/minikube/config.json ...
I0111 14:01:42.541073 3502553 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0111 14:01:42.541114 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:42.558855 3502553 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/xfhuang/.minikube/machines/minikube/id_rsa Username:docker}
I0111 14:01:42.594565 3502553 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0111 14:01:42.679608 3502553 start.go:128] duration metric: createHost completed in 23.1383396s
I0111 14:01:42.679623 3502553 start.go:83] releasing machines lock for "minikube", held for 23.1388633s
I0111 14:01:42.679727 3502553 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0111 14:01:42.822056 3502553 out.go:177] üåê  Found network options:
I0111 14:01:42.897216 3502553 out.go:177]     ‚ñ™ NO_PROXY=localhost,127.0.0.1,10.0.37.153,qua.io,10.7.116.12
W0111 14:01:42.988606 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988625 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988633 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988640 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988648 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988684 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988693 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988705 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988712 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
W0111 14:01:42.988719 3502553 proxy.go:119] fail to check proxy env: Error ip not in block
I0111 14:01:42.988832 3502553 ssh_runner.go:195] Run: systemctl --version
I0111 14:01:42.988888 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:42.988967 3502553 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.cn-hangzhou.aliyuncs.com/google_containers/
I0111 14:01:42.989025 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0111 14:01:43.012838 3502553 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/xfhuang/.minikube/machines/minikube/id_rsa Username:docker}
I0111 14:01:43.019495 3502553 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/xfhuang/.minikube/machines/minikube/id_rsa Username:docker}
I0111 14:01:43.175392 3502553 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0111 14:01:43.188459 3502553 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0111 14:01:43.188513 3502553 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0111 14:01:43.200470 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0111 14:01:43.220228 3502553 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0111 14:01:43.305889 3502553 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0111 14:01:43.375837 3502553 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0111 14:01:43.464956 3502553 ssh_runner.go:195] Run: sudo systemctl restart docker
I0111 14:01:45.097987 3502553 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.6330107s)
I0111 14:01:45.098055 3502553 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0111 14:01:45.198362 3502553 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0111 14:01:45.293665 3502553 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0111 14:01:45.309032 3502553 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0111 14:01:45.309112 3502553 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0111 14:01:45.312085 3502553 start.go:472] Will wait 60s for crictl version
I0111 14:01:45.312154 3502553 ssh_runner.go:195] Run: sudo crictl version
I0111 14:01:45.424627 3502553 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I0111 14:01:45.424700 3502553 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0111 14:01:45.457520 3502553 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0111 14:01:45.571944 3502553 out.go:204] üê≥  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...
I0111 14:01:45.680040 3502553 out.go:177]     ‚ñ™ env NO_PROXY=localhost,127.0.0.1,10.0.37.153,qua.io,10.7.116.12
I0111 14:01:45.755185 3502553 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0111 14:01:45.773578 3502553 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0111 14:01:45.776904 3502553 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0111 14:01:45.788970 3502553 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0111 14:01:45.806753 3502553 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I0111 14:01:45.806874 3502553 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0111 14:01:45.825026 3502553 docker.go:613] Got preloaded images: 
I0111 14:01:45.825035 3502553 docker.go:619] registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3 wasn't preloaded
I0111 14:01:45.825040 3502553 cache_images.go:88] LoadImages start: [registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8 registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0 registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3 registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5]
I0111 14:01:45.826144 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:45.826312 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:45.826321 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:45.826321 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:45.826415 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:45.826416 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:45.826427 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:45.826523 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:45.826554 3502553 image.go:134] retrieving image: registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:45.826851 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:45.826883 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:45.826913 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:45.826924 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:45.827185 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:45.827650 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:45.827760 3502553 image.go:177] daemon lookup for registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8: Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:46.075524 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:46.082639 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:46.094107 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0" does not exist at hash "a8a176a5d5d698f9409dc246f81fa69d37d4a2f4132ba5e62e72a78476b27f66" in container runtime
I0111 14:01:46.094181 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:46.094227 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.4-0
I0111 14:01:46.098327 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:46.100124 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:46.103744 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8" does not exist at hash "4873874c08efc72e9729683a83ffbb7502ee729e9a5ac097723806ea7fa13517" in container runtime
I0111 14:01:46.103771 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:46.103818 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
I0111 14:01:46.112450 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:46.116838 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0
I0111 14:01:46.117685 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/etcd_3.5.4-0
I0111 14:01:46.121220 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:46.123170 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:46.130658 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3" does not exist at hash "beaaf00edd38a6cb405376588e708084376a6786e722231dc8a1482730e0c041" in container runtime
I0111 14:01:46.130680 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:46.130725 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.3
I0111 14:01:46.131075 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3" does not exist at hash "5185b96f0becf59032b8e3646e99f84d9655dff3ac9e2605e0dc77f9c441ae4a" in container runtime
I0111 14:01:46.131091 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:46.131160 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
I0111 14:01:46.139814 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8
I0111 14:01:46.140640 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/pause_3.8
I0111 14:01:46.141578 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3" does not exist at hash "0346dbd74bcb9485bb4da1b33027094d79488470d8d1b9baa4d927db564e4fe0" in container runtime
I0111 14:01:46.141602 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:46.141645 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.3
I0111 14:01:46.141694 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/etcd_3.5.4-0: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/etcd_3.5.4-0: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/etcd_3.5.4-0': No such file or directory
I0111 14:01:46.141703 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0 --> /var/lib/minikube/images/etcd_3.5.4-0 (102160896 bytes)
I0111 14:01:46.177605 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3" does not exist at hash "60399923127581086e9029f30a0c9e3c88708efa8fc05d22d5e33887e7c0310a" in container runtime
I0111 14:01:46.177631 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:46.177678 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.3
I0111 14:01:46.204655 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5" does not exist at hash "6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" in container runtime
I0111 14:01:46.204660 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3
I0111 14:01:46.204678 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:46.204724 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0111 14:01:46.204725 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-proxy_v1.25.3
I0111 14:01:46.204772 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3
I0111 14:01:46.204809 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/coredns_v1.9.3
I0111 14:01:46.204850 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/pause_3.8: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/pause_3.8: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/pause_3.8': No such file or directory
I0111 14:01:46.204857 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8 --> /var/lib/minikube/images/pause_3.8 (311296 bytes)
I0111 14:01:46.204923 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3
I0111 14:01:46.204960 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-apiserver_v1.25.3
I0111 14:01:46.213358 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3
I0111 14:01:46.213429 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-controller-manager_v1.25.3
I0111 14:01:46.215656 3502553 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:46.227960 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-proxy_v1.25.3: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-proxy_v1.25.3: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/kube-proxy_v1.25.3': No such file or directory
I0111 14:01:46.227976 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3 --> /var/lib/minikube/images/kube-proxy_v1.25.3 (20268032 bytes)
I0111 14:01:46.245099 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5
I0111 14:01:46.245132 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/coredns_v1.9.3: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/coredns_v1.9.3: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/coredns_v1.9.3': No such file or directory
I0111 14:01:46.245145 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3 --> /var/lib/minikube/images/coredns_v1.9.3 (14839296 bytes)
I0111 14:01:46.245180 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/storage-provisioner_v5
I0111 14:01:46.245206 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-apiserver_v1.25.3: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-apiserver_v1.25.3: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/kube-apiserver_v1.25.3': No such file or directory
I0111 14:01:46.245213 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3 --> /var/lib/minikube/images/kube-apiserver_v1.25.3 (34241024 bytes)
I0111 14:01:46.245225 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-controller-manager_v1.25.3: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-controller-manager_v1.25.3: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/kube-controller-manager_v1.25.3': No such file or directory
I0111 14:01:46.245233 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3 --> /var/lib/minikube/images/kube-controller-manager_v1.25.3 (31264768 bytes)
I0111 14:01:46.246909 3502553 cache_images.go:116] "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3" needs transfer: "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3" does not exist at hash "6d23ec0e8b87eaaa698c3425c2c4d25f7329c587e9b39d967ab3f60048983912" in container runtime
I0111 14:01:46.246932 3502553 docker.go:293] Removing image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:46.246999 3502553 ssh_runner.go:195] Run: docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.3
I0111 14:01:46.258518 3502553 docker.go:260] Loading image: /var/lib/minikube/images/pause_3.8
I0111 14:01:46.258533 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/pause_3.8 | docker load"
I0111 14:01:46.268533 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/storage-provisioner_v5: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/storage-provisioner_v5: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/storage-provisioner_v5': No such file or directory
I0111 14:01:46.268555 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 --> /var/lib/minikube/images/storage-provisioner_v5 (9060352 bytes)
I0111 14:01:46.304510 3502553 cache_images.go:286] Loading image from: /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3
I0111 14:01:46.304594 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-scheduler_v1.25.3
I0111 14:01:46.882938 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.8 from cache
I0111 14:01:46.882943 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-scheduler_v1.25.3: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/kube-scheduler_v1.25.3: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/kube-scheduler_v1.25.3': No such file or directory
I0111 14:01:46.882956 3502553 docker.go:260] Loading image: /var/lib/minikube/images/storage-provisioner_v5
I0111 14:01:46.882957 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3 --> /var/lib/minikube/images/kube-scheduler_v1.25.3 (15801856 bytes)
I0111 14:01:46.882963 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/storage-provisioner_v5 | docker load"
I0111 14:01:47.900788 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/storage-provisioner_v5 | docker load": (1.0178102s)
I0111 14:01:47.900800 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 from cache
I0111 14:01:47.900866 3502553 docker.go:260] Loading image: /var/lib/minikube/images/coredns_v1.9.3
I0111 14:01:47.900873 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/coredns_v1.9.3 | docker load"
I0111 14:01:49.291626 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/coredns_v1.9.3 | docker load": (1.390717s)
I0111 14:01:49.291656 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.9.3 from cache
I0111 14:01:49.291693 3502553 docker.go:260] Loading image: /var/lib/minikube/images/kube-proxy_v1.25.3
I0111 14:01:49.291710 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-proxy_v1.25.3 | docker load"
I0111 14:01:51.034860 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-proxy_v1.25.3 | docker load": (1.7431315s)
I0111 14:01:51.034870 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.25.3 from cache
I0111 14:01:51.034882 3502553 docker.go:260] Loading image: /var/lib/minikube/images/kube-controller-manager_v1.25.3
I0111 14:01:51.034888 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-controller-manager_v1.25.3 | docker load"
I0111 14:01:53.309631 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-controller-manager_v1.25.3 | docker load": (2.2747231s)
I0111 14:01:53.309645 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.25.3 from cache
I0111 14:01:53.309660 3502553 docker.go:260] Loading image: /var/lib/minikube/images/kube-apiserver_v1.25.3
I0111 14:01:53.309666 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-apiserver_v1.25.3 | docker load"
I0111 14:01:54.722127 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-apiserver_v1.25.3 | docker load": (1.4124283s)
I0111 14:01:54.722156 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.25.3 from cache
I0111 14:01:54.722262 3502553 docker.go:260] Loading image: /var/lib/minikube/images/etcd_3.5.4-0
I0111 14:01:54.722281 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/etcd_3.5.4-0 | docker load"
I0111 14:01:59.540895 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/etcd_3.5.4-0 | docker load": (4.8185922s)
I0111 14:01:59.540910 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.4-0 from cache
I0111 14:01:59.540924 3502553 docker.go:260] Loading image: /var/lib/minikube/images/kube-scheduler_v1.25.3
I0111 14:01:59.540930 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-scheduler_v1.25.3 | docker load"
I0111 14:02:00.712798 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-scheduler_v1.25.3 | docker load": (1.1718371s)
I0111 14:02:00.712821 3502553 cache_images.go:315] Transferred and loaded /home/xfhuang/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.25.3 from cache
I0111 14:02:00.712895 3502553 cache_images.go:123] Successfully loaded all cached images
I0111 14:02:00.712906 3502553 cache_images.go:92] LoadImages completed in 14.8878554s
I0111 14:02:00.713024 3502553 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0111 14:02:00.783681 3502553 cni.go:95] Creating CNI manager for ""
I0111 14:02:00.783688 3502553 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0111 14:02:00.783695 3502553 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0111 14:02:00.783704 3502553 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I0111 14:02:00.783785 3502553 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0111 14:02:00.783840 3502553 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0111 14:02:00.783892 3502553 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I0111 14:02:00.794516 3502553 binaries.go:47] Didn't find k8s binaries: sudo ls /var/lib/minikube/binaries/v1.25.3: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/binaries/v1.25.3': No such file or directory

Initiating transfer...
I0111 14:02:00.794564 3502553 ssh_runner.go:195] Run: sudo mkdir -p /var/lib/minikube/binaries/v1.25.3
I0111 14:02:00.805213 3502553 download.go:101] Downloading: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/kubernetes-release/release/v1.25.3/bin/linux/amd64/kubeadm?checksum=file:https://kubernetes.oss-cn-hangzhou.aliyuncs.com/kubernetes-release/release/v1.25.3/bin/linux/amd64/kubeadm.sha256 -> /home/xfhuang/.minikube/cache/linux/amd64/v1.25.3/kubeadm
I0111 14:02:00.805214 3502553 download.go:101] Downloading: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/kubernetes-release/release/v1.25.3/bin/linux/amd64/kubelet?checksum=file:https://kubernetes.oss-cn-hangzhou.aliyuncs.com/kubernetes-release/release/v1.25.3/bin/linux/amd64/kubelet.sha256 -> /home/xfhuang/.minikube/cache/linux/amd64/v1.25.3/kubelet
I0111 14:02:00.805214 3502553 download.go:101] Downloading: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/kubernetes-release/release/v1.25.3/bin/linux/amd64/kubectl?checksum=file:https://kubernetes.oss-cn-hangzhou.aliyuncs.com/kubernetes-release/release/v1.25.3/bin/linux/amd64/kubectl.sha256 -> /home/xfhuang/.minikube/cache/linux/amd64/v1.25.3/kubectl
I0111 14:02:12.222331 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.25.3/kubectl
I0111 14:02:12.225312 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.25.3/kubectl: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.25.3/kubectl: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/binaries/v1.25.3/kubectl': No such file or directory
I0111 14:02:12.225324 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/linux/amd64/v1.25.3/kubectl --> /var/lib/minikube/binaries/v1.25.3/kubectl (45015040 bytes)
I0111 14:02:13.689455 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.25.3/kubeadm
I0111 14:02:13.692210 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.25.3/kubeadm: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.25.3/kubeadm: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/binaries/v1.25.3/kubeadm': No such file or directory
I0111 14:02:13.692224 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/linux/amd64/v1.25.3/kubeadm --> /var/lib/minikube/binaries/v1.25.3/kubeadm (43802624 bytes)
I0111 14:02:21.276118 3502553 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0111 14:02:21.289477 3502553 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.25.3/kubelet
I0111 14:02:21.292537 3502553 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.25.3/kubelet: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.25.3/kubelet: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/binaries/v1.25.3/kubelet': No such file or directory
I0111 14:02:21.292552 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/cache/linux/amd64/v1.25.3/kubelet --> /var/lib/minikube/binaries/v1.25.3/kubelet (114237464 bytes)
I0111 14:02:21.641410 3502553 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0111 14:02:21.652039 3502553 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (560 bytes)
I0111 14:02:21.673158 3502553 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0111 14:02:21.693408 3502553 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2100 bytes)
I0111 14:02:21.713296 3502553 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0111 14:02:21.727577 3502553 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0111 14:02:21.741419 3502553 certs.go:54] Setting up /home/xfhuang/.minikube/profiles/minikube for IP: 192.168.49.2
I0111 14:02:21.741450 3502553 certs.go:187] generating minikubeCA CA: /home/xfhuang/.minikube/ca.key
I0111 14:02:21.961733 3502553 crypto.go:156] Writing cert to /home/xfhuang/.minikube/ca.crt ...
I0111 14:02:21.961744 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/ca.crt: {Name:mk6e735fd0e51948968b6dc449c384524c5d2742 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:21.961900 3502553 crypto.go:164] Writing key to /home/xfhuang/.minikube/ca.key ...
I0111 14:02:21.961906 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/ca.key: {Name:mka7f8029066975802ce821058fef54258bd5f48 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:21.962000 3502553 certs.go:187] generating proxyClientCA CA: /home/xfhuang/.minikube/proxy-client-ca.key
I0111 14:02:22.143018 3502553 crypto.go:156] Writing cert to /home/xfhuang/.minikube/proxy-client-ca.crt ...
I0111 14:02:22.143032 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/proxy-client-ca.crt: {Name:mka21c9a70c76a0e229b2d8fb8867fcee9d52f20 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:22.143192 3502553 crypto.go:164] Writing key to /home/xfhuang/.minikube/proxy-client-ca.key ...
I0111 14:02:22.143217 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/proxy-client-ca.key: {Name:mkf6a0eb0cda5afdfe2ace1df23c275bc97fca47 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:22.143341 3502553 certs.go:302] generating minikube-user signed cert: /home/xfhuang/.minikube/profiles/minikube/client.key
I0111 14:02:22.143349 3502553 crypto.go:68] Generating cert /home/xfhuang/.minikube/profiles/minikube/client.crt with IP's: []
I0111 14:02:22.245831 3502553 crypto.go:156] Writing cert to /home/xfhuang/.minikube/profiles/minikube/client.crt ...
I0111 14:02:22.245843 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/client.crt: {Name:mk390c334ab14835f835b5433bfb09807d3229a1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:22.245967 3502553 crypto.go:164] Writing key to /home/xfhuang/.minikube/profiles/minikube/client.key ...
I0111 14:02:22.245972 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/client.key: {Name:mk61238a8184e3562c13d9c29ddcfd0a57a1a31e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:22.246084 3502553 certs.go:302] generating minikube signed cert: /home/xfhuang/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0111 14:02:22.246096 3502553 crypto.go:68] Generating cert /home/xfhuang/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0111 14:02:22.402632 3502553 crypto.go:156] Writing cert to /home/xfhuang/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0111 14:02:22.402643 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mkbc58e015979769f08cb3f9f0f6af7269180007 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:22.402763 3502553 crypto.go:164] Writing key to /home/xfhuang/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0111 14:02:22.402768 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mkf2dbe92243ad8bd24d439a8fe286e1350ff6a0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:22.402838 3502553 certs.go:320] copying /home/xfhuang/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/xfhuang/.minikube/profiles/minikube/apiserver.crt
I0111 14:02:22.402940 3502553 certs.go:324] copying /home/xfhuang/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/xfhuang/.minikube/profiles/minikube/apiserver.key
I0111 14:02:22.402971 3502553 certs.go:302] generating aggregator signed cert: /home/xfhuang/.minikube/profiles/minikube/proxy-client.key
I0111 14:02:22.402981 3502553 crypto.go:68] Generating cert /home/xfhuang/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0111 14:02:22.539144 3502553 crypto.go:156] Writing cert to /home/xfhuang/.minikube/profiles/minikube/proxy-client.crt ...
I0111 14:02:22.539155 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/proxy-client.crt: {Name:mk4b11108fb314f37d3121bd96649a55e9815509 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:25.716905 3502553 crypto.go:164] Writing key to /home/xfhuang/.minikube/profiles/minikube/proxy-client.key ...
I0111 14:02:29.426923 3502553 lock.go:35] WriteFile acquiring /home/xfhuang/.minikube/profiles/minikube/proxy-client.key: {Name:mk1dc9fd014f0fc8d999e3f572e7a0624e77122a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0111 14:02:29.585830 3502553 certs.go:388] found cert: /home/xfhuang/.minikube/certs/home/xfhuang/.minikube/certs/ca-key.pem (1675 bytes)
I0111 14:02:29.585857 3502553 certs.go:388] found cert: /home/xfhuang/.minikube/certs/home/xfhuang/.minikube/certs/ca.pem (1078 bytes)
I0111 14:02:29.585873 3502553 certs.go:388] found cert: /home/xfhuang/.minikube/certs/home/xfhuang/.minikube/certs/cert.pem (1123 bytes)
I0111 14:02:29.585887 3502553 certs.go:388] found cert: /home/xfhuang/.minikube/certs/home/xfhuang/.minikube/certs/key.pem (1679 bytes)
I0111 14:02:29.586299 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0111 14:02:29.615431 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0111 14:02:29.644119 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0111 14:02:29.673017 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0111 14:02:29.700387 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0111 14:02:29.728447 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0111 14:02:29.756281 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0111 14:02:29.784217 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0111 14:02:29.813236 3502553 ssh_runner.go:362] scp /home/xfhuang/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0111 14:02:29.844945 3502553 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0111 14:02:29.867963 3502553 ssh_runner.go:195] Run: openssl version
I0111 14:02:29.873172 3502553 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0111 14:02:29.887941 3502553 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0111 14:02:29.890800 3502553 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jan 11 06:02 /usr/share/ca-certificates/minikubeCA.pem
I0111 14:02:29.890851 3502553 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0111 14:02:29.894969 3502553 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0111 14:02:29.906253 3502553 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:8096 CPUs:4 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.7.20.12:5000 10.7.20.51] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/xfhuang:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0111 14:02:29.906350 3502553 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0111 14:02:29.941818 3502553 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0111 14:02:29.953021 3502553 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0111 14:02:29.963673 3502553 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I0111 14:02:29.963724 3502553 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0111 14:02:29.974319 3502553 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0111 14:02:29.974343 3502553 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0111 14:02:30.015085 3502553 kubeadm.go:317] [init] Using Kubernetes version: v1.25.3
I0111 14:02:30.015162 3502553 kubeadm.go:317] [preflight] Running pre-flight checks
I0111 14:02:30.119054 3502553 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I0111 14:02:30.119241 3502553 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0111 14:02:30.119408 3502553 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0111 14:02:30.245043 3502553 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0111 14:02:30.402926 3502553 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0111 14:02:30.403110 3502553 kubeadm.go:317] [certs] Using existing ca certificate authority
I0111 14:02:30.403205 3502553 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I0111 14:02:30.403320 3502553 kubeadm.go:317] [certs] Generating "apiserver-kubelet-client" certificate and key
I0111 14:02:30.537994 3502553 kubeadm.go:317] [certs] Generating "front-proxy-ca" certificate and key
I0111 14:02:30.682986 3502553 kubeadm.go:317] [certs] Generating "front-proxy-client" certificate and key
I0111 14:02:30.820929 3502553 kubeadm.go:317] [certs] Generating "etcd/ca" certificate and key
I0111 14:02:30.908725 3502553 kubeadm.go:317] [certs] Generating "etcd/server" certificate and key
I0111 14:02:30.908902 3502553 kubeadm.go:317] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0111 14:02:31.002657 3502553 kubeadm.go:317] [certs] Generating "etcd/peer" certificate and key
I0111 14:02:31.002860 3502553 kubeadm.go:317] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0111 14:02:31.180608 3502553 kubeadm.go:317] [certs] Generating "etcd/healthcheck-client" certificate and key
I0111 14:02:31.300610 3502553 kubeadm.go:317] [certs] Generating "apiserver-etcd-client" certificate and key
I0111 14:02:31.388269 3502553 kubeadm.go:317] [certs] Generating "sa" key and public key
I0111 14:02:31.388419 3502553 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0111 14:02:31.645233 3502553 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I0111 14:02:32.006313 3502553 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0111 14:02:32.209286 3502553 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0111 14:02:32.380189 3502553 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0111 14:02:32.389773 3502553 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0111 14:02:32.390407 3502553 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0111 14:02:32.390495 3502553 kubeadm.go:317] [kubelet-start] Starting the kubelet
I0111 14:02:32.482877 3502553 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0111 14:02:35.101961 3502553 out.go:204]     ‚ñ™ Booting up control plane ...
I0111 14:02:35.122366 3502553 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0111 14:02:35.122536 3502553 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0111 14:02:35.122717 3502553 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0111 14:02:35.123112 3502553 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0111 14:02:35.123639 3502553 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0111 14:03:12.489991 3502553 kubeadm.go:317] [kubelet-check] Initial timeout of 40s passed.
I0111 14:06:32.489823 3502553 kubeadm.go:317] 
I0111 14:06:32.489940 3502553 kubeadm.go:317] Unfortunately, an error has occurred:
I0111 14:06:32.490035 3502553 kubeadm.go:317] 	timed out waiting for the condition
I0111 14:06:32.490039 3502553 kubeadm.go:317] 
I0111 14:06:32.490105 3502553 kubeadm.go:317] This error is likely caused by:
I0111 14:06:32.490186 3502553 kubeadm.go:317] 	- The kubelet is not running
I0111 14:06:32.490382 3502553 kubeadm.go:317] 	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
I0111 14:06:32.490391 3502553 kubeadm.go:317] 
I0111 14:06:32.490577 3502553 kubeadm.go:317] If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
I0111 14:06:32.490625 3502553 kubeadm.go:317] 	- 'systemctl status kubelet'
I0111 14:06:32.490702 3502553 kubeadm.go:317] 	- 'journalctl -xeu kubelet'
I0111 14:06:32.490712 3502553 kubeadm.go:317] 
I0111 14:06:32.490926 3502553 kubeadm.go:317] Additionally, a control plane component may have crashed or exited when started by the container runtime.
I0111 14:06:32.491081 3502553 kubeadm.go:317] To troubleshoot, list all containers using your preferred container runtimes CLI.
I0111 14:06:32.491432 3502553 kubeadm.go:317] Here is one example how you may list all running Kubernetes containers by using crictl:
I0111 14:06:32.491822 3502553 kubeadm.go:317] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
I0111 14:06:32.491957 3502553 kubeadm.go:317] 	Once you have found the failing container, you can inspect its logs with:
I0111 14:06:32.492116 3502553 kubeadm.go:317] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
I0111 14:06:32.495384 3502553 kubeadm.go:317] W0111 06:02:30.008906    1708 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I0111 14:06:32.495783 3502553 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0111 14:06:32.496159 3502553 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0111 14:06:32.496399 3502553 kubeadm.go:317] error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
I0111 14:06:32.496653 3502553 kubeadm.go:317] To see the stack trace of this error execute with --v=5 or higher
W0111 14:06:32.497010 3502553 out.go:239] üí¢  initialization failed, will try again: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.25.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0111 06:02:30.008906    1708 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

I0111 14:06:32.497182 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0111 14:06:35.018859 3502553 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (2.5216584s)
I0111 14:06:35.018926 3502553 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0111 14:06:35.031418 3502553 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I0111 14:06:35.031469 3502553 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0111 14:06:35.041904 3502553 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0111 14:06:35.041930 3502553 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0111 14:06:35.078438 3502553 kubeadm.go:317] [init] Using Kubernetes version: v1.25.3
I0111 14:06:35.078514 3502553 kubeadm.go:317] [preflight] Running pre-flight checks
I0111 14:06:35.165273 3502553 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I0111 14:06:35.165479 3502553 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0111 14:06:35.165618 3502553 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0111 14:06:35.276030 3502553 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0111 14:06:35.402229 3502553 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0111 14:06:35.402442 3502553 kubeadm.go:317] [certs] Using existing ca certificate authority
I0111 14:06:35.402574 3502553 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I0111 14:06:35.402745 3502553 kubeadm.go:317] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I0111 14:06:35.402846 3502553 kubeadm.go:317] [certs] Using existing front-proxy-ca certificate authority
I0111 14:06:35.402952 3502553 kubeadm.go:317] [certs] Using existing front-proxy-client certificate and key on disk
I0111 14:06:35.403051 3502553 kubeadm.go:317] [certs] Using existing etcd/ca certificate authority
I0111 14:06:35.403187 3502553 kubeadm.go:317] [certs] Using existing etcd/server certificate and key on disk
I0111 14:06:35.403280 3502553 kubeadm.go:317] [certs] Using existing etcd/peer certificate and key on disk
I0111 14:06:35.403402 3502553 kubeadm.go:317] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I0111 14:06:35.403540 3502553 kubeadm.go:317] [certs] Using existing apiserver-etcd-client certificate and key on disk
I0111 14:06:35.403596 3502553 kubeadm.go:317] [certs] Using the existing "sa" key
I0111 14:06:35.403681 3502553 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0111 14:06:35.403831 3502553 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I0111 14:06:35.560265 3502553 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0111 14:06:35.645925 3502553 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0111 14:06:35.713140 3502553 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0111 14:06:35.723585 3502553 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0111 14:06:35.724343 3502553 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0111 14:06:35.724457 3502553 kubeadm.go:317] [kubelet-start] Starting the kubelet
I0111 14:06:35.817153 3502553 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0111 14:06:35.893897 3502553 out.go:204]     ‚ñ™ Booting up control plane ...
I0111 14:06:35.894268 3502553 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0111 14:06:35.894503 3502553 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0111 14:06:35.894642 3502553 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0111 14:06:35.894708 3502553 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0111 14:06:35.894902 3502553 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0111 14:07:15.823139 3502553 kubeadm.go:317] [kubelet-check] Initial timeout of 40s passed.
I0111 14:10:35.824257 3502553 kubeadm.go:317] 
I0111 14:10:35.824349 3502553 kubeadm.go:317] Unfortunately, an error has occurred:
I0111 14:10:35.824434 3502553 kubeadm.go:317] 	timed out waiting for the condition
I0111 14:10:35.824443 3502553 kubeadm.go:317] 
I0111 14:10:35.824518 3502553 kubeadm.go:317] This error is likely caused by:
I0111 14:10:35.824569 3502553 kubeadm.go:317] 	- The kubelet is not running
I0111 14:10:35.824769 3502553 kubeadm.go:317] 	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
I0111 14:10:35.824776 3502553 kubeadm.go:317] 
I0111 14:10:35.824968 3502553 kubeadm.go:317] If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
I0111 14:10:35.825049 3502553 kubeadm.go:317] 	- 'systemctl status kubelet'
I0111 14:10:35.825099 3502553 kubeadm.go:317] 	- 'journalctl -xeu kubelet'
I0111 14:10:35.825103 3502553 kubeadm.go:317] 
I0111 14:10:35.825300 3502553 kubeadm.go:317] Additionally, a control plane component may have crashed or exited when started by the container runtime.
I0111 14:10:35.825450 3502553 kubeadm.go:317] To troubleshoot, list all containers using your preferred container runtimes CLI.
I0111 14:10:35.825638 3502553 kubeadm.go:317] Here is one example how you may list all running Kubernetes containers by using crictl:
I0111 14:10:35.825827 3502553 kubeadm.go:317] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
I0111 14:10:35.825967 3502553 kubeadm.go:317] 	Once you have found the failing container, you can inspect its logs with:
I0111 14:10:35.826160 3502553 kubeadm.go:317] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
I0111 14:10:35.828949 3502553 kubeadm.go:317] W0111 06:06:35.074029    4381 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I0111 14:10:35.829201 3502553 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0111 14:10:35.829412 3502553 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0111 14:10:35.829583 3502553 kubeadm.go:317] error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
I0111 14:10:35.829714 3502553 kubeadm.go:317] To see the stack trace of this error execute with --v=5 or higher
I0111 14:10:35.829756 3502553 kubeadm.go:398] StartCluster complete in 8m5.9235087s
I0111 14:10:35.829809 3502553 cri.go:52] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0111 14:10:35.829868 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0111 14:10:35.854219 3502553 cri.go:87] found id: ""
I0111 14:10:35.854274 3502553 logs.go:274] 0 containers: []
W0111 14:10:35.854281 3502553 logs.go:276] No container was found matching "kube-apiserver"
I0111 14:10:35.854287 3502553 cri.go:52] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0111 14:10:35.854342 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0111 14:10:35.877162 3502553 cri.go:87] found id: ""
I0111 14:10:35.877171 3502553 logs.go:274] 0 containers: []
W0111 14:10:35.877175 3502553 logs.go:276] No container was found matching "etcd"
I0111 14:10:35.877179 3502553 cri.go:52] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0111 14:10:35.877227 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0111 14:10:35.900683 3502553 cri.go:87] found id: ""
I0111 14:10:35.900694 3502553 logs.go:274] 0 containers: []
W0111 14:10:35.900698 3502553 logs.go:276] No container was found matching "coredns"
I0111 14:10:35.900702 3502553 cri.go:52] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0111 14:10:35.900834 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0111 14:10:35.925181 3502553 cri.go:87] found id: ""
I0111 14:10:35.925210 3502553 logs.go:274] 0 containers: []
W0111 14:10:35.925214 3502553 logs.go:276] No container was found matching "kube-scheduler"
I0111 14:10:35.925219 3502553 cri.go:52] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0111 14:10:35.925291 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0111 14:10:35.949378 3502553 cri.go:87] found id: ""
I0111 14:10:35.949388 3502553 logs.go:274] 0 containers: []
W0111 14:10:35.949393 3502553 logs.go:276] No container was found matching "kube-proxy"
I0111 14:10:35.949397 3502553 cri.go:52] listing CRI containers in root : {State:all Name:kubernetes-dashboard Namespaces:[]}
I0111 14:10:35.949468 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kubernetes-dashboard
I0111 14:10:35.976921 3502553 cri.go:87] found id: ""
I0111 14:10:35.976932 3502553 logs.go:274] 0 containers: []
W0111 14:10:35.976936 3502553 logs.go:276] No container was found matching "kubernetes-dashboard"
I0111 14:10:35.976940 3502553 cri.go:52] listing CRI containers in root : {State:all Name:storage-provisioner Namespaces:[]}
I0111 14:10:35.976995 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=storage-provisioner
I0111 14:10:36.001014 3502553 cri.go:87] found id: ""
I0111 14:10:36.001025 3502553 logs.go:274] 0 containers: []
W0111 14:10:36.001030 3502553 logs.go:276] No container was found matching "storage-provisioner"
I0111 14:10:36.001035 3502553 cri.go:52] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0111 14:10:36.001087 3502553 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0111 14:10:36.024164 3502553 cri.go:87] found id: ""
I0111 14:10:36.024175 3502553 logs.go:274] 0 containers: []
W0111 14:10:36.024180 3502553 logs.go:276] No container was found matching "kube-controller-manager"
I0111 14:10:36.024187 3502553 logs.go:123] Gathering logs for Docker ...
I0111 14:10:36.024194 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0111 14:10:36.063471 3502553 logs.go:123] Gathering logs for container status ...
I0111 14:10:36.063488 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0111 14:10:36.089254 3502553 logs.go:123] Gathering logs for kubelet ...
I0111 14:10:36.089267 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0111 14:10:36.134220 3502553 logs.go:123] Gathering logs for dmesg ...
I0111 14:10:36.134237 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0111 14:10:36.148087 3502553 logs.go:123] Gathering logs for describe nodes ...
I0111 14:10:36.148101 3502553 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0111 14:10:36.194464 3502553 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
W0111 14:10:36.194487 3502553 out.go:369] Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.25.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0111 06:06:35.074029    4381 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
W0111 14:10:36.194518 3502553 out.go:239] 
W0111 14:10:36.194769 3502553 out.go:239] üí£  Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.25.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0111 06:06:35.074029    4381 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

W0111 14:10:36.194888 3502553 out.go:239] 
W0111 14:10:36.195576 3502553 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0111 14:10:36.395950 3502553 out.go:177] 
W0111 14:10:36.479387 3502553 out.go:239] ‚ùå  Exiting due to K8S_KUBELET_NOT_RUNNING: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.25.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0111 06:06:35.074029    4381 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

W0111 14:10:36.479558 3502553 out.go:239] üí°  Suggestion: Check output of 'journalctl -xeu kubelet', try passing --extra-config=kubelet.cgroup-driver=systemd to minikube start
W0111 14:10:36.479617 3502553 out.go:239] üçø  Related issue: https://github.com/kubernetes/minikube/issues/4172
I0111 14:10:36.587607 3502553 out.go:177] 


==> Docker <==
-- Logs begin at Wed 2023-01-11 06:01:33 UTC, end at Wed 2023-01-11 06:12:49 UTC. --
Jan 11 06:10:38 minikube dockerd[703]: time="2023-01-11T06:10:38.349313100Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:38 minikube dockerd[703]: time="2023-01-11T06:10:38.349388800Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:38 minikube dockerd[703]: time="2023-01-11T06:10:38.440456100Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:38 minikube dockerd[703]: time="2023-01-11T06:10:38.440504700Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:38 minikube dockerd[703]: time="2023-01-11T06:10:38.444582700Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:38 minikube dockerd[703]: time="2023-01-11T06:10:38.536420300Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:40 minikube dockerd[703]: time="2023-01-11T06:10:40.433087000Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:40 minikube dockerd[703]: time="2023-01-11T06:10:40.433131900Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:40 minikube dockerd[703]: time="2023-01-11T06:10:40.511318800Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:45 minikube dockerd[703]: time="2023-01-11T06:10:45.410112200Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:45 minikube dockerd[703]: time="2023-01-11T06:10:45.410258900Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:10:45 minikube dockerd[703]: time="2023-01-11T06:10:45.590168300Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:06 minikube dockerd[703]: time="2023-01-11T06:11:06.433965400Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:06 minikube dockerd[703]: time="2023-01-11T06:11:06.434116300Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:06 minikube dockerd[703]: time="2023-01-11T06:11:06.520073500Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:07 minikube dockerd[703]: time="2023-01-11T06:11:07.405831600Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:07 minikube dockerd[703]: time="2023-01-11T06:11:07.405890900Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:07 minikube dockerd[703]: time="2023-01-11T06:11:07.502294600Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:09 minikube dockerd[703]: time="2023-01-11T06:11:09.357955000Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:09 minikube dockerd[703]: time="2023-01-11T06:11:09.358090100Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:09 minikube dockerd[703]: time="2023-01-11T06:11:09.444888000Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:12 minikube dockerd[703]: time="2023-01-11T06:11:12.337840300Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:12 minikube dockerd[703]: time="2023-01-11T06:11:12.337870000Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:12 minikube dockerd[703]: time="2023-01-11T06:11:12.427082200Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:33 minikube dockerd[703]: time="2023-01-11T06:11:33.345917900Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:33 minikube dockerd[703]: time="2023-01-11T06:11:33.345966300Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:33 minikube dockerd[703]: time="2023-01-11T06:11:33.442124900Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:35 minikube dockerd[703]: time="2023-01-11T06:11:35.428159000Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:35 minikube dockerd[703]: time="2023-01-11T06:11:35.428206800Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:35 minikube dockerd[703]: time="2023-01-11T06:11:35.517202400Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:36 minikube dockerd[703]: time="2023-01-11T06:11:36.361885800Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:36 minikube dockerd[703]: time="2023-01-11T06:11:36.361931200Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:36 minikube dockerd[703]: time="2023-01-11T06:11:36.450428700Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:40 minikube dockerd[703]: time="2023-01-11T06:11:40.409225800Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:40 minikube dockerd[703]: time="2023-01-11T06:11:40.409269700Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:40 minikube dockerd[703]: time="2023-01-11T06:11:40.532423400Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:11:59 minikube dockerd[703]: time="2023-01-11T06:11:59.351034600Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:59 minikube dockerd[703]: time="2023-01-11T06:11:59.351374100Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:11:59 minikube dockerd[703]: time="2023-01-11T06:11:59.447870100Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:04 minikube dockerd[703]: time="2023-01-11T06:12:04.353330200Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:04 minikube dockerd[703]: time="2023-01-11T06:12:04.354427700Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:04 minikube dockerd[703]: time="2023-01-11T06:12:04.430923300Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:05 minikube dockerd[703]: time="2023-01-11T06:12:05.449879900Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:05 minikube dockerd[703]: time="2023-01-11T06:12:05.449947900Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:05 minikube dockerd[703]: time="2023-01-11T06:12:05.564792700Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:06 minikube dockerd[703]: time="2023-01-11T06:12:06.351797100Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:06 minikube dockerd[703]: time="2023-01-11T06:12:06.352119100Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:06 minikube dockerd[703]: time="2023-01-11T06:12:06.447183500Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:27 minikube dockerd[703]: time="2023-01-11T06:12:27.343211700Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:27 minikube dockerd[703]: time="2023-01-11T06:12:27.343287200Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:27 minikube dockerd[703]: time="2023-01-11T06:12:27.437972600Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:31 minikube dockerd[703]: time="2023-01-11T06:12:31.354535900Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:31 minikube dockerd[703]: time="2023-01-11T06:12:31.354625400Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:31 minikube dockerd[703]: time="2023-01-11T06:12:31.438583600Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:34 minikube dockerd[703]: time="2023-01-11T06:12:34.364904700Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:34 minikube dockerd[703]: time="2023-01-11T06:12:34.364961200Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:34 minikube dockerd[703]: time="2023-01-11T06:12:34.487539200Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jan 11 06:12:37 minikube dockerd[703]: time="2023-01-11T06:12:37.367379700Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:12:37 minikube dockerd[703]: time="2023-01-11T06:12:37.367751300Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"
Jan 11 06:12:37 minikube dockerd[703]: time="2023-01-11T06:12:37.461880300Z" level=error msg="Handler for POST /v1.40/images/create returned error: Get \"https://k8s.gcr.io/v2/\": dial tcp 74.125.203.82:443: i/o timeout"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID


==> describe nodes <==

==> dmesg <==
[Jan10 09:05] PCI: Fatal: No config space access function found
[  +0.011803] PCI: System does not support PCI
[  +0.118121] kvm: already loaded the other module
[  +0.113163] hv_utils: cannot register PTP clock: 0
[  +0.337216] WSL (1) ERROR: StartTimeSyncAgent:3314: /dev/ptp0 not found - kernel must be built with CONFIG_PTP_1588_CLOCK
[  +0.279228] FS-Cache: Duplicate cookie detected
[  +0.000385] FS-Cache: O-cookie c=000000005834ffcb [p=000000004c51fc06 fl=222 nc=0 na=1]
[  +0.000683] FS-Cache: O-cookie d=00000000fb6c4b44 n=000000007603b22c
[  +0.000715] FS-Cache: O-key=[10] '34323934393337333833'
[  +0.000731] FS-Cache: N-cookie c=00000000b0a0abd6 [p=000000004c51fc06 fl=2 nc=0 na=1]
[  +0.000919] FS-Cache: N-cookie d=00000000fb6c4b44 n=00000000783bb9d5
[  +0.000722] FS-Cache: N-key=[10] '34323934393337333833'
[  +3.940922] 9pnet_virtio: no channels available for device drvfs
[  +0.001056] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.113020] 9pnet_virtio: no channels available for device drvfs
[  +0.001386] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +5.632533] 9pnet_virtio: no channels available for device drvfs
[  +0.000932] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.107702] 9pnet_virtio: no channels available for device drvfs
[  +0.000587] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +7.428076] systemd-journald[57]: File /var/log/journal/11e47d8c8b2466150a15119a62b4385a/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jan10 09:29] systemd-journald[57]: File /var/log/journal/11e47d8c8b2466150a15119a62b4385a/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.


==> kernel <==
 06:12:51 up 21:07,  0 users,  load average: 0.70, 0.80, 0.99
Linux minikube 5.10.102.1-microsoft-hxf-standard-WSL2+ #1 SMP Fri May 13 08:50:33 CST 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"


==> kubelet <==
-- Logs begin at Wed 2023-01-11 06:01:33 UTC, end at Wed 2023-01-11 06:12:51 UTC. --
Jan 11 06:12:45 minikube kubelet[4511]: E0111 06:12:45.942682    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.043770    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.144604    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.244885    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.345871    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.447093    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: I0111 06:12:46.455735    4511 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.456132    4511 eviction_manager.go:256] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.456275    4511 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.547243    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.648174    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.749205    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.849445    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:46 minikube kubelet[4511]: E0111 06:12:46.950341    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.051203    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.152214    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.252943    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.354019    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.454976    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.555779    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.656736    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.757656    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.857741    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:47 minikube kubelet[4511]: E0111 06:12:47.958712    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.059557    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.160549    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.261742    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.362542    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.463340    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.564495    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.665258    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.766259    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.867471    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:48 minikube kubelet[4511]: E0111 06:12:48.968706    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.069563    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.170447    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.271280    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.371588    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.472210    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.573176    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.674282    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.774984    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.875276    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:49 minikube kubelet[4511]: E0111 06:12:49.976171    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.076780    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.177584    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.277935    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.378357    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.479397    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.580611    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.680844    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.781208    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.881580    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:50 minikube kubelet[4511]: E0111 06:12:50.982340    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:51 minikube kubelet[4511]: E0111 06:12:51.083296    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:51 minikube kubelet[4511]: E0111 06:12:51.183392    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:51 minikube kubelet[4511]: E0111 06:12:51.283745    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:51 minikube kubelet[4511]: E0111 06:12:51.383881    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:51 minikube kubelet[4511]: E0111 06:12:51.484712    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"
Jan 11 06:12:51 minikube kubelet[4511]: E0111 06:12:51.585303    4511 kubelet.go:2448] "Error getting node" err="node \"minikube\" not found"

